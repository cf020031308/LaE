# Label as Equilibrium: A Performance Booster for Graph Neural Networks on Node Classification

## Usage

```
usage: main.py [-h] [--runs RUNS] [--gpu GPU] [--split SPLIT] [--lr LR]
               [--dropout DROPOUT] [--n-layers N_LAYERS]
               [--weight-decay WEIGHT_DECAY]
               [--early-stop-epochs EARLY_STOP_EPOCHS]
               [--max-epochs MAX_EPOCHS] [--hidden HIDDEN] [--heads HEADS]
               [--alpha ALPHA] [--theta THETA] [--drop-state DROP_STATE]
               [--drop-features DROP_FEATURES] [--input-label INPUT_LABEL]
               [--for-iter FOR_ITER] [--back-iter BACK_ITER] [--inductive]
               [--original-reuse] [--terminate-back] [--cache]
               method dataset

positional arguments:
  method                MLP | SGC | GCN | IGNN | EIGNN | GIN | SAGE | GAT
                        | GCNII | JKNet
  dataset               cora | citeseer | pubmed | flickr | arxiv | yelp |
                        reddit | ...

options:
  -h, --help            show this help message and exit
  --runs RUNS           Default: 1
  --gpu GPU             Default: 0
  --split SPLIT         Ratio of labels for training. Set to 0 to use
                        default split (if any) or 0.6. With an integer x
                        the dataset is splitted like Cora with the
                        training set be composed by x samples per class.
                        Default: 0
  --lr LR               Learning Rate. Default: 0.01
  --dropout DROPOUT     Default: 0
  --n-layers N_LAYERS   Default: 2
  --weight-decay WEIGHT_DECAY
                        Default: 0
  --early-stop-epochs EARLY_STOP_EPOCHS
                        Maximum epochs until stop when accuracy
                        decreasing. Default: 100
  --max-epochs MAX_EPOCHS
                        Maximum epochs. Default: 1000
  --hidden HIDDEN       Dimension of hidden representations and implicit
                        state. Default: 64
  --heads HEADS         Number of attention heads for GAT. Default: 1
  --alpha ALPHA         Hyperparameter for GCNII. Default: 0.5
  --theta THETA         Hyperparameter for GCNII. Default: 1.0
  --drop-state DROP_STATE
                        Ratio of known labels to mask in input. Default: 0
  --drop-features DROP_FEATURES
                        Ratio of node features to mask in input. Default:
                        0
  --input-label INPUT_LABEL
                        Ratio of known labels for input. Default: 0
  --for-iter FOR_ITER   Iterations to produce state in forward-pass.
                        Default: 0
  --back-iter BACK_ITER
                        Iterations to accumulate vjp in backward-pass (0:
                        disabled; -1: adaptive). Default: 0
  --inductive           Enable the inductive setting
  --original-reuse      apply the original version (with label leakage) of
                        label reuse
  --terminate-back      terminate backward pass if the norm of vjp expands
  --cache               accelerate iterations with cache
```

For example, if you want to run MLP on the Cora dataset with its default split on gpu `cuda:3` for 5 runs, execute

```bash
python3 main.py MLP cora --split 0 --gpu 3 --runs 5
```

## Reproducibility

Files in `scripts/` folder are scripts that reproduce experiments in our article.

* `run_weekday` runs experiment on the Weekday dataset, producing data for Figure 3 in Section 4.1
* `run_chain` runs experiment on the Chains dataset, producing data for Figure 4 in Section 4.2
* `run_ablation` runs experiments on GraphSAGE with LaI, LR, and LaE, producing data for Table 1 in Section 4.3
* `run_gnn` runs experiments on different homophilic GNNs with LaE, producing data for Table 2 in Section 4.3
* `run_hetgnn` runs experiments on different heterophilic GNNs with LaE, producing data for Table 3 in Section 4.3
* `run_wellpose` runs experiments on LR+ and LaE, producing data for Figure C.5 in Appendix C

## Datasets

The Chains dataset and the Transitional Chains dataset are generated by code modified from [IGNN](https://github.com/SwiftieH/IGNN).
The Weekday dataset is generated by code from [3ference](https://github.com/cf020031308/3ference).
The BGP dataset is from [PathNet](https://github.com/Sunefei/PathNet).
The genius dataset is from [LINKX](https://github.com/CUAI/Non-Homophily-Large-Scale).
Other datasets used in our paper, including Chameleon, Squirrel, Flickr, Pubmed, Amazon Photo, Coauthor CS, and etc, are retrieved [PyG](https://github.com/pyg-team/pytorch_geometric).

Datasets that did not appear in our paper can also be retrieved by our code for further exploration, with the help of PyG, and [OGB](https://github.com/snap-stanford/ogb).

## Baselines

We implement all methods except IGNN and EIGNN with PyTorch, PyG, and Scikit-learn, if you want to run one of them, install PyTorch, Scikit-learn, PyG, and OGB first then execute `main.py`.

To run IGNN and EIGNN, you need to clone [this commit of EIGNN](https://github.com/liu-jc/EIGNN/tree//6a2c8e73c11bfebc8614d955226dbae600cc8dfc) (and install its dependencies) then place our `main.py` into the cloned folder.

## Citation

```bibtex
```
